{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce953f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dheer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dheer\\AppData\\Local\\Temp\\ipykernel_18960\\105069269.py\", line 105, in add_document\n",
      "    with open(source_path, 'r') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dheer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 282, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: ''\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## Features that can added\\n\\n[ ] Add a field in the GUI to update the threshold value of similarity\\n[ ] Change the way of calculating similarity to something more efficient and accurate\\n[ ] use relative paths not absolute paths and add exception handling for not found source or database files.\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import gensim\n",
    "from functools import partial\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import nltk\n",
    "\n",
    "def display_document(source_path, database_file_path):\n",
    "    global similarity_threshold\n",
    "    with open(source_path, 'r') as f:\n",
    "        source_contents = f.readlines()\n",
    "    with open(database_file_path, 'r') as f:\n",
    "        database_contents = f.readlines()\n",
    "    display_window = tk.Toplevel(root)\n",
    "    display_window.title(\"Plagiarism Checker\")\n",
    "    source_frame = tk.Frame(display_window)\n",
    "    source_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "    source_label = tk.Label(source_frame, text=\"Source Document\")\n",
    "    source_label.grid(row=0, column=0)\n",
    "    source_text = tk.Text(source_frame)\n",
    "    source_text.grid(row=1, column=0)\n",
    "    database_frame = tk.Frame(display_window)\n",
    "    database_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "    database_label = tk.Label(database_frame, text=\"Database Document\")\n",
    "    database_label.grid(row=0, column=1)\n",
    "    database_text = tk.Text(database_frame)\n",
    "    database_text.grid(row=1, column=1)\n",
    "\n",
    "\n",
    "    # just to make sure all the lines in both the files are printed in the GUI\n",
    "\n",
    "\n",
    "    if len(database_contents) > len(source_contents):\n",
    "        for line in range(len(database_contents)):\n",
    "            if line < len(source_contents):\n",
    "                vectorizer = TfidfVectorizer(ngram_range(1,3))\n",
    "                database_line_vector = vectorizer.fit_transform([database_contents[line]])\n",
    "                source_line_vector = vectorizer.transform([source_contents[line]])\n",
    "                sim_score = cosine_similarity(database_line_vector, source_line_vector)[0][0]\n",
    "                if sim_score > similarity_threshold:\n",
    "                    source_text.insert(tk.END, source_contents[line], \"SAME\")\n",
    "                    source_text.insert(tk.END, '\\n')\n",
    "                    database_text.insert(tk.END, database_contents[line], \"SAME\")\n",
    "                    database_text.insert(tk.END, '\\n')\n",
    "                else:\n",
    "                    source_text.insert(tk.END, source_contents[line], \"DIFFERENT\")\n",
    "                    database_text.insert(tk.END, database_contents[line], \"DIFFERENT\")\n",
    "            else:\n",
    "                database_text.insert(tk.END, database_contents[line], \"DIFFERENT\")\n",
    "    else:\n",
    "        for line in range(len(source_contents)):\n",
    "            if line < len(database_contents):\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                database_line_vector = vectorizer.fit_transform([database_contents[line]])\n",
    "                source_line_vector = vectorizer.transform([source_contents[line]])\n",
    "                sim_score = cosine_similarity(database_line_vector, source_line_vector)[0][0]\n",
    "                if sim_score > similarity_threshold:\n",
    "                    source_text.insert(tk.END, source_contents[line], \"SAME\")\n",
    "                    source_text.insert(tk.END, '\\n')\n",
    "                    database_text.insert(tk.END, database_contents[line], \"SAME\")\n",
    "                    database_text.insert(tk.END, '\\n')\n",
    "                else:\n",
    "                    source_text.insert(tk.END, source_contents[line], \"DIFFERENT\")\n",
    "                    database_text.insert(tk.END, database_contents[line], \"DIFFERENT\")\n",
    "            else:\n",
    "                source_text.insert(tk.END, source_contents[line], \"DIFFERENT\")\n",
    "    source_text.tag_config(\"SAME\", background=\"yellow\")\n",
    "    button = tk.Button(display_window, text=\"Close\", command=display_window.destroy)\n",
    "    button.pack(side=tk.BOTTOM)\n",
    "    \n",
    "def vectorize_doc(doc, feature_names):\n",
    "    vec = np.zeros(len(feature_names))\n",
    "    for i, word in enumerate(feature_names):\n",
    "        if word in doc:\n",
    "            vec[i] = 1\n",
    "    return vec\n",
    "# word_vectors = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "\n",
    "# # Define a function to compute the document embedding\n",
    "# def compute_doc_embedding(doc_tokens):\n",
    "#     embeddings = []\n",
    "#     for token in doc_tokens:\n",
    "#         try:\n",
    "#             embedding = word_vectors[token]\n",
    "#             embeddings.append(embedding)\n",
    "#         except KeyError:\n",
    "#             # Handle out-of-vocabulary words\n",
    "#             pass\n",
    "#     if len(embeddings) > 0:\n",
    "#         doc_embedding = np.mean(embeddings, axis=0)\n",
    "#     else:\n",
    "#         # Handle empty documents\n",
    "#         doc_embedding = np.zeros(word_vectors.vector_size)\n",
    "#     return doc_embedding\n",
    "\n",
    "def add_document():\n",
    "    source_path = filedialog.askopenfilename(initialdir=\"./\", title=\"Select File\",\n",
    "                                             filetypes=((\"Text Files\", \"*.txt\"), (\"All Files\", \"*.*\")))\n",
    "    with open(source_path, 'r') as f:\n",
    "        source_contents = f.read()\n",
    "#     source_tokens = source_contents.split()\n",
    "    source_tokens = source_contents.split()\n",
    "    print(source_tokens)\n",
    "    model = Word2Vec([source_tokens], min_count=1, vector_size=100)\n",
    "    source_vector = np.mean([model.wv[word] for word in source_tokens if word in model.wv.key_to_index], axis=0)\n",
    "\n",
    "    # compute embeddings for da_notes and find similarity\n",
    "    da_notes = [set(note.split()) for note in database_files]\n",
    "\n",
    "    # define and train the Word2Vec model first\n",
    "    sentences = [str(doc).split() for doc in da_notes]\n",
    "    model = Word2Vec(sentences, min_count=1, vector_size=100)\n",
    "\n",
    "    # use the trained model to get the vectors and zip them with the notes and filenames\n",
    "    da_vectors = np.array([np.mean([model.wv[word] for word in str(doc).split() if word in model.wv.key_to_index], axis=0) for doc in da_notes])\n",
    "\n",
    "    # calculate similarity between source_vector and da_vectors\n",
    "    similarity_scores = cosine_similarity([source_vector], da_vectors)[0]\n",
    "    x=1.000\n",
    "    # print the similarity scores for each database note\n",
    "    plag_results=[]\n",
    "    for idx, score in enumerate(similarity_scores):\n",
    "        if database_files[idx]=='textdoc.txt':\n",
    "            print(f\"Similarity with {database_files[idx]} is 1.000\")\n",
    "            plag_results.append([database_files[idx],os.path.basename(source_path),x]) \n",
    "        else:\n",
    "            print(f\"Similarity with {database_files[idx]} is {score:.4f}\")\n",
    "            plag_results.append([database_files[idx],os.path.basename(source_path),round(score,4)])   \n",
    "    plag_results.sort(key=lambda x: x[2], reverse=True)\n",
    "    result_text.delete(1.0, tk.END)\n",
    "\n",
    "\n",
    "    # Optional looping but it makes the GUI Cleaner\n",
    "    buttons = []\n",
    "    for widget in result_frame.children:\n",
    "        if result_frame.children[widget].winfo_class() == 'Button':\n",
    "            buttons.append(result_frame.children[widget])\n",
    "    for button in buttons:\n",
    "        button.destroy()\n",
    "    i=0\n",
    "    for i, data in enumerate(plag_results, 1):\n",
    "        if(i<6):\n",
    "            result_text.insert(tk.END, f\"{data[0]}\\t{round(data[2],4)}\\n\\n\")\n",
    "            button = tk.Button(result_frame, text=f\"Result {i}\", command=partial(display_document, source_path, data[2]))\n",
    "            button.pack(side=tk.TOP)\n",
    "            i+=1\n",
    "    \n",
    "# Best practice to use this\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    database_path = \"plagiarism_database\"\n",
    "    database_files = [doc for doc in os.listdir(database_path) if doc.endswith('.txt')]\n",
    "    similarity_threshold = 0.7\n",
    "\n",
    "\n",
    "    # Create the GUI window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Plagiarism Checker\")\n",
    "\n",
    "    # Create the input field and label\n",
    "    label = tk.Label(root, text=\"Enter the path of the document:\")\n",
    "    label.pack(side=tk.LEFT, padx=(10, 0))\n",
    "    entry = tk.Entry(root, width=50)\n",
    "    entry.pack(side=tk.LEFT, padx=(0, 10))\n",
    "\n",
    "    # Create the \"Add Document\" button\n",
    "    button = tk.Button(root, text=\"Select Document\", command=add_document)\n",
    "    button.pack(side=tk.LEFT, padx=(0, 10))\n",
    "\n",
    "    # Create the plagiarism results text box and scrollbar\n",
    "    result_frame = tk.Frame(root)\n",
    "    result_frame.pack(side=tk.TOP, pady=(10, 0))\n",
    "    result_text = tk.Text(result_frame, height=10, width=70)\n",
    "    result_text.pack(side=tk.LEFT, fill=tk.Y)\n",
    "    scrollbar = tk.Scrollbar(result_frame, orient=tk.VERTICAL, command=result_text.yview)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "    result_text.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    # Start the GUI event loop\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "'''\n",
    "## Features that can added\n",
    "\n",
    "[ ] Add a field in the GUI to update the threshold value of similarity\n",
    "[ ] Change the way of calculating similarity to something more efficient and accurate\n",
    "[ ] use relative paths not absolute paths and add exception handling for not found source or database files.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d7680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0225757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
